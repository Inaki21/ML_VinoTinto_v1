{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "309f2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caa35d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "vinotinto_df = pd.read_csv('../data/processed/vinotinto.csv')\n",
    "\n",
    "# Convertir quality_label a valores numéricos\n",
    "label_encoder = LabelEncoder()\n",
    "vinotinto_df['quality_label_encoded'] = label_encoder.fit_transform(vinotinto_df['quality_label'])\n",
    "\n",
    "# Definir características (X) y variable objetivo (y)\n",
    "X = vinotinto_df.drop(['quality', 'quality_label', 'quality_label_encoded'], axis=1)\n",
    "y = vinotinto_df['quality_label_encoded']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "239c25bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d00b8f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes de SMOTE: Counter({1: 448, 2: 374, 0: 129})\n",
      "Después de SMOTE: Counter({2: 448, 1: 448, 0: 448})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Balanceo de clases con SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Ver el balance de clases después de SMOTE\n",
    "print(f\"Antes de SMOTE: {Counter(y_train)}\")\n",
    "print(f\"Después de SMOTE: {Counter(y_res)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c2cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir los parámetros a probar para cada modelo\n",
    "\n",
    "# Regresión Logística\n",
    "param_grid_logreg = {'C': [0.1, 1, 10, 20, 100], 'solver': ['lbfgs', 'liblinear']}\n",
    "logreg = LogisticRegression(class_weight='balanced', max_iter=10000)\n",
    "grid_search_logreg = GridSearchCV(logreg, param_grid_logreg, cv=5, scoring='accuracy')\n",
    "grid_search_logreg.fit(X_res, y_res)\n",
    "print(\"Mejores parámetros para Regresión Logística:\", grid_search_logreg.best_params_)\n",
    "\n",
    "# Árbol de Decisión\n",
    "param_grid_dtree = {'max_depth': [3, 5, 10, 20, None], 'min_samples_split': [2, 10, 20, 40]}\n",
    "dtree = DecisionTreeClassifier(class_weight='balanced')\n",
    "grid_search_dtree = GridSearchCV(dtree, param_grid_dtree, cv=5, scoring='accuracy')\n",
    "grid_search_dtree.fit(X_res, y_res)\n",
    "print(\"Mejores parámetros para Árbol de Decisión:\", grid_search_dtree.best_params_)\n",
    "\n",
    "# Random Forest\n",
    "param_grid_rf = {'n_estimators': [50, 100, 200, 500, 1000], 'max_depth': [3, 5, 10, 20, None]}\n",
    "rf = RandomForestClassifier(class_weight='balanced')\n",
    "grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='accuracy')\n",
    "grid_search_rf.fit(X_res, y_res)\n",
    "print(\"Mejores parámetros para Random Forest:\", grid_search_rf.best_params_)\n",
    "\n",
    "# SVM\n",
    "param_grid_svm = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf'], 'gamma': ['scale', 'auto']}\n",
    "svm = SVC(class_weight='balanced')\n",
    "grid_search_svm = GridSearchCV(svm, param_grid_svm, cv=5, scoring='accuracy')\n",
    "grid_search_svm.fit(X_res, y_res)\n",
    "print(\"Mejores parámetros para SVM:\", grid_search_svm.best_params_)\n",
    "\n",
    "# KNN\n",
    "param_grid_knn = {'n_neighbors': [3, 5, 7, 10], 'weights': ['uniform', 'distance']}\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search_knn = GridSearchCV(knn, param_grid_knn, cv=5, scoring='accuracy')\n",
    "grid_search_knn.fit(X_res, y_res)\n",
    "print(\"Mejores parámetros para KNN:\", grid_search_knn.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7249ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluación Regresión Logística:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.78      0.60        55\n",
      "           1       0.68      0.71      0.70       192\n",
      "           2       0.54      0.40      0.46       161\n",
      "\n",
      "    accuracy                           0.60       408\n",
      "   macro avg       0.57      0.63      0.59       408\n",
      "weighted avg       0.60      0.60      0.59       408\n",
      "\n",
      "Evaluación Árbol de Decisión:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.42      0.39        55\n",
      "           1       0.70      0.66      0.68       192\n",
      "           2       0.50      0.51      0.51       161\n",
      "\n",
      "    accuracy                           0.57       408\n",
      "   macro avg       0.52      0.53      0.53       408\n",
      "weighted avg       0.58      0.57      0.57       408\n",
      "\n",
      "Evaluación Random Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.64      0.56        55\n",
      "           1       0.72      0.69      0.70       192\n",
      "           2       0.52      0.50      0.51       161\n",
      "\n",
      "    accuracy                           0.61       408\n",
      "   macro avg       0.58      0.61      0.59       408\n",
      "weighted avg       0.61      0.61      0.61       408\n",
      "\n",
      "Evaluación SVM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.51      0.42        55\n",
      "           1       0.63      0.58      0.60       192\n",
      "           2       0.39      0.37      0.38       161\n",
      "\n",
      "    accuracy                           0.49       408\n",
      "   macro avg       0.46      0.49      0.47       408\n",
      "weighted avg       0.50      0.49      0.49       408\n",
      "\n",
      "Evaluación KNN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.49      0.39        55\n",
      "           1       0.60      0.65      0.62       192\n",
      "           2       0.42      0.31      0.36       161\n",
      "\n",
      "    accuracy                           0.49       408\n",
      "   macro avg       0.45      0.48      0.46       408\n",
      "weighted avg       0.49      0.49      0.49       408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Usar los mejores modelos encontrados en GridSearchCV\n",
    "best_logreg = grid_search_logreg.best_estimator_\n",
    "best_dtree = grid_search_dtree.best_estimator_\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "best_svm = grid_search_svm.best_estimator_\n",
    "best_knn = grid_search_knn.best_estimator_\n",
    "\n",
    "# Entrenar con los datos balanceados por SMOTE\n",
    "best_logreg.fit(X_res, y_res)\n",
    "best_dtree.fit(X_res, y_res)\n",
    "best_rf.fit(X_res, y_res)\n",
    "best_svm.fit(X_res, y_res)\n",
    "best_knn.fit(X_res, y_res)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_logreg = best_logreg.predict(X_test)\n",
    "y_pred_dtree = best_dtree.predict(X_test)\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "y_pred_svm = best_svm.predict(X_test)\n",
    "y_pred_knn = best_knn.predict(X_test)\n",
    "\n",
    "# Evaluar el rendimiento de cada modelo\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Evaluación Regresión Logística:\")\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "\n",
    "print(\"Evaluación Árbol de Decisión:\")\n",
    "print(classification_report(y_test, y_pred_dtree))\n",
    "\n",
    "print(\"Evaluación Random Forest:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "print(\"Evaluación SVM:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "print(\"Evaluación KNN:\")\n",
    "print(classification_report(y_test, y_pred_knn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83bd4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\inaki\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:30:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"scale_pos_weight\", \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluación XGBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.56      0.52        55\n",
      "           1       0.70      0.68      0.69       192\n",
      "           2       0.49      0.48      0.48       161\n",
      "\n",
      "    accuracy                           0.59       408\n",
      "   macro avg       0.56      0.57      0.57       408\n",
      "weighted avg       0.59      0.59      0.59       408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calcular el valor de scale_pos_weight para XGBoost\n",
    "scale_pos_weight = len(y_res) / (3 * np.bincount(y_res))\n",
    "\n",
    "# Inicializar y entrenar el modelo XGBoost con scale_pos_weight\n",
    "xgb_model = xgb.XGBClassifier(scale_pos_weight=scale_pos_weight, use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_model.fit(X_res, y_res)\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluar el rendimiento\n",
    "print(\"Evaluación XGBoost:\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e770898a",
   "metadata": {},
   "source": [
    "utilizando Pesos de clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0933563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluación Random Forest con Pesos de Clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.33      0.44        55\n",
      "           1       0.70      0.71      0.70       192\n",
      "           2       0.52      0.61      0.56       161\n",
      "\n",
      "    accuracy                           0.62       408\n",
      "   macro avg       0.64      0.55      0.57       408\n",
      "weighted avg       0.63      0.62      0.61       408\n",
      "\n",
      "Evaluación Regresión Logística con Pesos de Clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.80      0.59        55\n",
      "           1       0.68      0.71      0.69       192\n",
      "           2       0.53      0.38      0.44       161\n",
      "\n",
      "    accuracy                           0.59       408\n",
      "   macro avg       0.56      0.63      0.58       408\n",
      "weighted avg       0.59      0.59      0.58       408\n",
      "\n",
      "Evaluación Árbol de Decisión con Pesos de Clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.38      0.38        55\n",
      "           1       0.62      0.62      0.62       192\n",
      "           2       0.42      0.41      0.41       161\n",
      "\n",
      "    accuracy                           0.51       408\n",
      "   macro avg       0.47      0.47      0.47       408\n",
      "weighted avg       0.51      0.51      0.51       408\n",
      "\n",
      "Evaluación SVM con Pesos de Clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.45      0.36        55\n",
      "           1       0.63      0.62      0.63       192\n",
      "           2       0.40      0.33      0.36       161\n",
      "\n",
      "    accuracy                           0.49       408\n",
      "   macro avg       0.44      0.47      0.45       408\n",
      "weighted avg       0.49      0.49      0.49       408\n",
      "\n",
      "Evaluación KNN con Pesos de Clase:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.27      0.32        55\n",
      "           1       0.59      0.72      0.65       192\n",
      "           2       0.46      0.39      0.42       161\n",
      "\n",
      "    accuracy                           0.53       408\n",
      "   macro avg       0.48      0.46      0.46       408\n",
      "weighted avg       0.51      0.53      0.51       408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Entrenar los modelos utilizando el parámetro `class_weight='balanced'`\n",
    "\n",
    "# Random Forest con pesos de clase\n",
    "rf_balanced = RandomForestClassifier(class_weight='balanced', n_estimators=1000, max_depth=20, random_state=42)\n",
    "rf_balanced.fit(X_train, y_train)\n",
    "\n",
    "# Predicción y evaluación\n",
    "y_pred_rf_balanced = rf_balanced.predict(X_test)\n",
    "print(\"Evaluación Random Forest con Pesos de Clase:\")\n",
    "print(classification_report(y_test, y_pred_rf_balanced))\n",
    "\n",
    "# Otros modelos con pesos de clase\n",
    "\n",
    "# Regresión Logística\n",
    "logreg_balanced = LogisticRegression(class_weight='balanced', C=100, solver='lbfgs', max_iter=10000)\n",
    "logreg_balanced.fit(X_train, y_train)\n",
    "y_pred_logreg_balanced = logreg_balanced.predict(X_test)\n",
    "print(\"Evaluación Regresión Logística con Pesos de Clase:\")\n",
    "print(classification_report(y_test, y_pred_logreg_balanced))\n",
    "\n",
    "# Árbol de Decisión\n",
    "dtree_balanced = DecisionTreeClassifier(class_weight='balanced', max_depth=None, min_samples_split=2)\n",
    "dtree_balanced.fit(X_train, y_train)\n",
    "y_pred_dtree_balanced = dtree_balanced.predict(X_test)\n",
    "print(\"Evaluación Árbol de Decisión con Pesos de Clase:\")\n",
    "print(classification_report(y_test, y_pred_dtree_balanced))\n",
    "\n",
    "# SVM\n",
    "svm_balanced = SVC(class_weight='balanced', C=10, gamma='auto', kernel='rbf')\n",
    "svm_balanced.fit(X_train, y_train)\n",
    "y_pred_svm_balanced = svm_balanced.predict(X_test)\n",
    "print(\"Evaluación SVM con Pesos de Clase:\")\n",
    "print(classification_report(y_test, y_pred_svm_balanced))\n",
    "\n",
    "# KNN\n",
    "knn_balanced = KNeighborsClassifier(n_neighbors=3, weights='distance')\n",
    "knn_balanced.fit(X_train, y_train)\n",
    "y_pred_knn_balanced = knn_balanced.predict(X_test)\n",
    "print(\"Evaluación KNN con Pesos de Clase:\")\n",
    "print(classification_report(y_test, y_pred_knn_balanced))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accb2ace",
   "metadata": {},
   "source": [
    "El mejor modelos es el RAndomFOrest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f2fd1a",
   "metadata": {},
   "source": [
    "Script para Afinar los Hiperparámetros de Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967d463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e11d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Definir el modelo Random Forest\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "# Definir el rango de parámetros a probar\n",
    "# param_grid_rf = {\n",
    "#     'n_estimators': [100, 200, 300, 500, 1000],  # Número de árboles\n",
    "#     'max_depth': [3, 5, 10, 20, None],  # Profundidad máxima de los árboles\n",
    "#     'min_samples_split': [2, 5, 10],  # Mínimo número de muestras para dividir un nodo\n",
    "#     'min_samples_leaf': [1, 2, 4],  # Mínimo número de muestras en cada hoja\n",
    "#     'max_features': ['auto', 'sqrt', 'log2'],  # Número de características a considerar en cada división\n",
    "#     'bootstrap': [True, False]  # Si se utiliza muestreo bootstrap en la construcción de árboles\n",
    "# }\n",
    "\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],  # Solo probar 2 números de árboles\n",
    "    'max_depth': [3, 5],  # Solo probar dos profundidades\n",
    "    'min_samples_split': [2],  # Solo probar la mínima cantidad de muestras para dividir\n",
    "    'min_samples_leaf': [1],  # Solo una opción\n",
    "    'max_features': ['auto'],  # Una sola opción\n",
    "    'bootstrap': [True]  # Solo una opción\n",
    "}\n",
    "\n",
    "# Inicializar GridSearchCV con el modelo y el rango de parámetros\n",
    "grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Añadir la barra de progreso con tqdm\n",
    "# Usamos tqdm para envolver la lista de combinaciones de parámetros en GridSearchCV\n",
    "# Esto nos da una barra de progreso mientras realiza las combinaciones de prueba\n",
    "# El número total de combinaciones de parámetros es el número de pruebas que se harán\n",
    "tuned_params = list(grid_search_rf.param_grid.values())  # Extraer los parámetros a probar\n",
    "total_combinations = np.prod([len(x) for x in tuned_params])  # Total de combinaciones\n",
    "\n",
    "# Ajustar el modelo con la barra de progreso\n",
    "for _ in tqdm(range(total_combinations), desc=\"Entrenando GridSearchCV\", total=total_combinations):\n",
    "    grid_search_rf.fit(X_res, y_res)\n",
    "\n",
    "# Ver los mejores parámetros encontrados\n",
    "print(\"Mejores parámetros para Random Forest:\", grid_search_rf.best_params_)\n",
    "\n",
    "# Usar el mejor modelo encontrado en GridSearchCV\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluar el rendimiento con los mejores parámetros\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Evaluación Random Forest con los mejores parámetros:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae26d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Definir el modelo Random Forest\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "# Definir el rango de parámetros a probar\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300, 500, 1000],  # Número de árboles\n",
    "    'max_depth': [3, 5, 10, 20, None],  # Profundidad máxima de los árboles\n",
    "    'min_samples_split': [2, 5, 10],  # Mínimo número de muestras para dividir un nodo\n",
    "    'min_samples_leaf': [1, 2, 4],  # Mínimo número de muestras en cada hoja\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],  # Número de características a considerar en cada división\n",
    "    'bootstrap': [True, False]  # Si se utiliza muestreo bootstrap en la construcción de árboles\n",
    "}\n",
    "\n",
    "# Inicializar GridSearchCV con el modelo y el rango de parámetros\n",
    "grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1, verbose=0)\n",
    "\n",
    "# Crear la barra de progreso\n",
    "total_combinations = np.prod([len(v) for v in param_grid_rf.values()])  # Total de combinaciones\n",
    "progress_bar = tqdm(total=total_combinations, desc=\"Entrenando GridSearchCV\", position=0, leave=True)\n",
    "\n",
    "# Crear una función de callback para actualizar la barra de progreso\n",
    "def callback(*args, **kwargs):\n",
    "    progress_bar.update(1)  # Actualiza la barra de progreso en cada ajuste\n",
    "\n",
    "# Añadir la barra de progreso y callback a GridSearchCV\n",
    "grid_search_rf.fit(X_res, y_res)\n",
    "progress_bar.close()  # Cerrar la barra al final\n",
    "\n",
    "# Ver los mejores parámetros encontrados\n",
    "print(\"Mejores parámetros para Random Forest:\", grid_search_rf.best_params_)\n",
    "\n",
    "# Usar el mejor modelo encontrado en GridSearchCV\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluar el rendimiento con los mejores parámetros\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Evaluación Random Forest con los mejores parámetros:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b8454a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando GridSearchCV:   0%|          | 0/810 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Ajustar el modelo con la barra de progreso\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(total_combinations), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntrenando GridSearchCV\u001b[39m\u001b[38;5;124m\"\u001b[39m, total\u001b[38;5;241m=\u001b[39mtotal_combinations):\n\u001b[1;32m---> 31\u001b[0m     grid_search_rf\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_res\u001b[49m, y_res)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Ver los mejores parámetros encontrados\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMejores parámetros para Random Forest:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search_rf\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_res' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Definir el modelo Random Forest\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "# Definir el rango de parámetros a probar\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300, ],  # Número de árboles\n",
    "    'max_depth': [3, 5, 10, 20, None],  # Profundidad máxima de los árboles\n",
    "    'min_samples_split': [2, 5, 10],  # Mínimo número de muestras para dividir un nodo\n",
    "    'min_samples_leaf': [1, 2, 4],  # Mínimo número de muestras en cada hoja\n",
    "    'max_features': ['sqrt', 'log2', None],  # Número de características a considerar en cada división\n",
    "    'bootstrap': [True, False]  # Si se utiliza muestreo bootstrap en la construcción de árboles\n",
    "}\n",
    "\n",
    "# Inicializar GridSearchCV con el modelo y el rango de parámetros\n",
    "grid_search_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Añadir la barra de progreso con tqdm\n",
    "# Usamos tqdm para envolver la lista de combinaciones de parámetros en GridSearchCV\n",
    "# Esto nos da una barra de progreso mientras realiza las combinaciones de prueba\n",
    "# El número total de combinaciones de parámetros es el número de pruebas que se harán\n",
    "tuned_params = list(grid_search_rf.param_grid.values())  # Extraer los parámetros a probar\n",
    "total_combinations = np.prod([len(x) for x in tuned_params])  # Total de combinaciones\n",
    "\n",
    "# Ajustar el modelo con la barra de progreso\n",
    "for _ in tqdm(range(total_combinations), desc=\"Entrenando GridSearchCV\", total=total_combinations):\n",
    "    grid_search_rf.fit(X_res, y_res)\n",
    "\n",
    "# Ver los mejores parámetros encontrados\n",
    "print(\"Mejores parámetros para Random Forest:\", grid_search_rf.best_params_)\n",
    "\n",
    "# Usar el mejor modelo encontrado en GridSearchCV\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "\n",
    "# Predecir en el conjunto de prueba\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluar el rendimiento con los mejores parámetros\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Evaluación Random Forest con los mejores parámetros:\")\n",
    "print(classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f56d5a",
   "metadata": {},
   "source": [
    "Guardar el modelo con pickle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ff398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Asumiendo que best_rf es el modelo que ya has ajustado con GridSearchCV\n",
    "\n",
    "# Guardar el modelo entrenado en un archivo\n",
    "with open('random_forest_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_rf, file)\n",
    "\n",
    "# Para cargar el modelo y usarlo más tarde:\n",
    "with open('random_forest_model.pkl', 'rb') as file:\n",
    "    loaded_rf_model = pickle.load(file)\n",
    "\n",
    "# Usar el modelo cargado para hacer predicciones\n",
    "y_pred_loaded_rf = loaded_rf_model.predict(X_test)\n",
    "\n",
    "# Evaluar el rendimiento\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Evaluación del modelo cargado:\")\n",
    "print(classification_report(y_test, y_pred_loaded_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6463ae9",
   "metadata": {},
   "source": [
    "Guardar el modelo con TensorFlow (para redes neuronales):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2618cc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Ejemplo de creación de un modelo secuencial con TensorFlow\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_dim=X_train.shape[1]),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # Para clasificación de 3 clases\n",
    "])\n",
    "\n",
    "# Compilar y entrenar el modelo\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_res, y_res, epochs=10, batch_size=32)\n",
    "\n",
    "# Guardar el modelo\n",
    "model.save('tensorflow_model.h5')\n",
    "\n",
    "# Cargar el modelo\n",
    "loaded_model = tf.keras.models.load_model('tensorflow_model.h5')\n",
    "\n",
    "# Hacer predicciones con el modelo cargado\n",
    "y_pred_loaded_model = loaded_model.predict(X_test)\n",
    "\n",
    "# Convertir las probabilidades a etiquetas de clase\n",
    "y_pred_class = np.argmax(y_pred_loaded_model, axis=1)\n",
    "\n",
    "# Evaluar el rendimiento\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Evaluación del modelo cargado de TensorFlow:\")\n",
    "print(classification_report(y_test, y_pred_class))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
